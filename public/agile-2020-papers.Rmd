---
title: "Reproducibility Review AGILE 2020"
author: "Daniel NÃ¼st"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: yes
    self_contained: true
  #pdf_document:
  #  toc: yes
params:
  private_info: yes
---

This document includes some scripts and text analysis to support the reproducibility review at the [AGILE conference 2020](https://agile-online.org/conference-2020), in Chania, Greece - _"Geospatial Technologies: seeding the future"_.

Find out more about the reproducible publications at AGILE [here](https://doi.org/10.17605/OSF.IO/PHMCE) all about the review process at https://osf.io/eg4qx/.
You can see the source code of the analysis in the R Markdown file [`agile-2020-papers.Rmd`](agile-2020-papers.Rmd).

```{r load_libraries, message=FALSE, warning=FALSE, include=FALSE}
library("pdftools")
library("stringr")
library("tidyverse")
library("tidytext")
library("wordcloud")
library("RColorBrewer")
library("here")
library("quanteda")
library("googledrive")
library("kableExtra")
library("httr")
library("xml2")
library("rvest")
library("tidyr")
library("ggplot2")
```

```{r seed, echo=FALSE}
set.seed(23) # 23rd AGILE!
```

## Submitted papers

```{r local_paths, echo=FALSE}
fp_path <- here::here("2020", "full-paper")
sp_path <- here::here("2020", "short-paper")
po_path <- here::here("2020", "poster")
#dir.create(po_path, recursive = TRUE)

submissions_path <- here::here("2020")
```

### Submission metadata

```{r submissions, echo=FALSE}
if(is.na(Sys.getenv("easychair_username", unset = NA)) || is.na(Sys.getenv("easychair_password", unset = NA))) {
  stop("Provide login details for EasyChair (e.g., in a file `.Renviron`) in environment variables",
       "`easychair_username` and `easychair_password`")
}

# with help from https://github.com/kaytwo/easierchair/blob/master/scrape_easychair.py
# https://stackoverflow.com/questions/23202522/r-httr-post-request-for-signing-in
login_response <- NULL
if(is.null(login_response)) {
  login_response <- POST(url = "https://easychair.org/account/verify",
                         body = list(name = Sys.getenv("easychair_username"),
                                     password = Sys.getenv("easychair_password")),
                         encode = "form")
}

submission_page <- httr::GET(url = "https://easychair.org/conferences/submissions?a=24268236")
submission_html <- xml2::read_html(submission_page)
submission_table_full <- html_node(submission_html, "#ec\\:table1")
# remove first header row and set names manually later, because the vertical table headers are miages anyway
xml_remove(xml_child(html_node(submission_table_full, "thead")))
submission_table <- rvest::html_table(x = submission_table_full,
                                      fill = TRUE)
names(submission_table) <- c("id", "authors", "title", "information", "paper", "assigment", "update", "type", "time", "decision")
submission_table$id <- str_pad(submission_table$id, width = 3, side = "left", pad = "0")
submission_table <- submission_table  %>%
  mutate_if(is.character, list(~na_if(.,"")))

links <- html_nodes(submission_table_full, "a[href]") %>% html_attr("href")
submission_table$information <- paste("https://easychair.org",
                                      links[str_detect(links, pattern = "submission_view")], sep = "")
# FIXME no. 033 has no file
submission_table$paper <- R.utils::insert(paste("https://easychair.org",
                                                links[str_detect(links, pattern = "download")],
                                                sep = ""),
                                          ats = which(submission_table$id == "033"),
                                          values = NA)

submission_table %>%
  arrange(id) %>%
  kable() %>%
  kable_styling("striped") %>%
  scroll_box(height = "480px")
```

### Download files from EasyChair

```{r download_easychair, echo=FALSE}
for (i in 1:nrow(submission_table)) {
  if(is.na(submission_table[i,]$paper)) {
    cat("No paper URL for ", i, "\n")
    next
  }
  
  current <- submission_table[i,]
  filename <- file.path(submissions_path, paste0(current$id, ".pdf"))
  if(!file.exists(filename)) {
    httr::GET(url = submission_table[i,]$paper,
              httr::write_disk(path = filename,
                               overwrite = TRUE))
  }
}

submission_files <- dir(path = submissions_path, pattern = ".pdf$", full.names = TRUE)

submission_table <- left_join(submission_table,
                              tibble("id" = str_match(submission_files, pattern = "([:digit:]*)\\.pdf")[,2],
                                     "file" = submission_files),
                              by = "id")
```

### Load texts

The text is extracted from PDFs and it is processed to create a [tidy](https://www.jstatsoft.org/article/view/v059i10) data structure without [stop words](https://en.wikipedia.org/wiki/Stop_words).
The stop words include specific words, which might be included in the page header, abbreviations, and terms particular to scientific articles, such as `figure`.

```{r tidy_data, echo=FALSE}
texts <- list()
for (i in 1:nrow(submission_table)) {
  current <- submission_table[i,]
  #cat("Reading ", current$file, "\n")
  the_text <- NA
  if(!is.na(current$file)) {
    the_text <- pdf_text(current$file)
    the_text <- str_c(the_text, collapse = TRUE)
  }
  
  names(the_text) <- current$id
  texts <- c(texts, the_text)
}

pages <- lapply(submission_table$file, function(f) {
  if(is.na(f))
    NA
  else pdf_info(f)$pages
})

tidy_texts <- tibble(id = submission_table$id,
                     path = submission_table$file,
                     type = submission_table$type,
                     text = unlist(texts),
                     pages = pages)

# create a table of all words
all_words <- tidy_texts %>%
  select(id,
         type,
         text) %>%
  unnest_tokens(word, text)

# remove stop words and remove numbers
my_stop_words <- tibble(
  word = c(
    "et",
    "al",
    "fig",
    "e.g",
    "i.e",
    "http",
    "ing",
    "pp",
    "figure",
    "based",
    "university",
    "table"
  ),
  lexicon = "agile"
)

all_stop_words <- stop_words %>%
  bind_rows(my_stop_words)
suppressWarnings({
  no_numbers <- all_words %>%
    filter(is.na(as.numeric(word)))
})

no_stop_words <- no_numbers %>%
  anti_join(all_stop_words, by = "word")

total_words = nrow(all_words)
after_cleanup = nrow(no_stop_words)
```

About `r round(after_cleanup/total_words * 100)`&nbsp;% of the words are considered stop words.

The following tables shows how many words and non-stop words each document has, sorted by number of non-stop words.
The `id` is built from the file name plus a prefix:
for full papers, it is the left-padded submission number and the prefix `fp_`;
<!--for short papers and posters, it is the submission number included in the file name and the prefixes `sp_` and `po_` respectively.-->

```{r stop_words, echo=FALSE, message=FALSE, warning=FALSE}
nsw_per_doc <- no_stop_words %>%
  group_by(id) %>%
  summarise(words = n()) %>%
  rename(`non-stop words` = words)

words_per_doc <- all_words %>%
  group_by(id, type) %>%
  summarise(words = n())

inner_join(words_per_doc, nsw_per_doc) %>%
  bind_rows(tibble(id = "Total", words = sum(words_per_doc$words), `non-stop words` = sum(nsw_per_doc$`non-stop words`))) %>%
  kable() %>%
  kable_styling("striped", full_width = FALSE) %>%
  row_spec(nrow(nsw_per_doc) + 1, bold = TRUE) %>%
  scroll_box(height = "240px")
```

### Which papers include a "Data and Software Availability" section?

According the the [AGILE Reproducible Paper Guidelines](https://osf.io/c8gtq/), all authors must add a _Data and Software Availability_ section to their paper.
The guidelines are not mandatory yet in 2020, but let's see how many authors did include the statement.

```{r pdfgrep, echo=FALSE, eval=FALSE}
# Quick version with `pdfgrep`
cmd <- paste("pdfgrep", "-e 'Data and Software Availability'", "-i", "-A 3", "2020/*/*")
output <- system(cmd, intern = TRUE)
print(cmd)
print(output)
```

```{r dasa_section, echo=FALSE}
dasa_pattern <- regex("Data and Software Availability", ignore_case = TRUE)
tidy_texts <- tidy_texts %>%
  mutate(has_dasa = str_detect(tidy_texts$text, pattern = dasa_pattern))

dasa_count <- tidy_texts %>% filter(has_dasa) %>% nrow()

excerpt_length <- 800
dasa_texts <- tidy_texts %>%
  filter(has_dasa) %>%
  mutate(dasa_start = str_locate(.data$text, pattern = dasa_pattern)[,1]) %>%
  mutate(dasa_text = str_sub(.data$text, start = dasa_start, end = dasa_start + excerpt_length)) %>%
  select(id, type, dasa_text)
```

`r dasa_count` papers have the section in question, that is `r round(dasa_count/nrow(submission_table) * 100)`&nbsp;% of all submissions.
Here are the statistics per submission type:

```{r dasa_statistics, echo=FALSE}
dasa_stats <- tidy_texts %>%
  filter(has_dasa) %>%
  group_by(type, .drop = FALSE) %>%
  summarise(n = n())

dasa_stats <- left_join(dasa_stats, tidy_texts %>%
                          group_by(type, .drop = FALSE) %>%
                          summarise(submissions = n()),
                        by = "type")

dasa_stats <- dasa_stats %>%
  mutate(`%` = round(n/submissions*100, digits = 1))

dasa_stats %>%
  arrange(desc(n)) %>%
  kable() %>%
  kable_styling("striped")
```

The first `r excerpt_length` characters of these sections are as follows.

```{r dasa_section_table, echo=FALSE}
dasa_texts %>%
  arrange(id) %>%
  kable() %>%
  kable_styling("striped") %>%
  scroll_box(height = "320px")
```


### Wordstem analysis

```{r wordstem_data, include=FALSE}
wordstems <- no_stop_words %>%
  mutate(wordstem = quanteda::char_wordstem(no_stop_words$word))

countPapersUsingWordstem <- function(the_word) {
  sapply(the_word, function(w) {
    wordstems %>%
      filter(wordstem == w) %>%
      group_by(id) %>%
      count %>%
      nrow
  })
}

top_wordstems <- wordstems %>%
  group_by(wordstem) %>%
  tally %>%
  arrange(desc(n)) %>%
  head(20) %>%
  mutate(`# papers` = countPapersUsingWordstem(wordstem)) %>%
  mutate(`% papers` = round(countPapersUsingWordstem(wordstem)/nrow(submission_table) * 100)) %>%
  add_column(place = c(1:nrow(.)), .before = 0)

minimum_occurence <- 100
cloud_wordstems <- wordstems %>%
  group_by(wordstem) %>%
  tally %>%
  filter(n >= minimum_occurence) %>%
  arrange(desc(n))
```

For the following table and figure, the word stems were extracted based on a stemming algorithm from package [`quanteda`](https://cran.r-project.org/package=quanteda).
The word cloud is based on `r length(unique(cloud_wordstems$wordstem))` unique words occuring each at least `r minimum_occurence` times, all in all occuring `r sum(cloud_wordstems$n)` times which comprises `r round(sum(cloud_wordstems$n)/ nrow(no_stop_words) * 100)`&nbsp;% of non-stop words.

```{r top_wordstems, echo=FALSE}
top_wordstems %>%
  kable() %>%
  kable_styling("striped") %>%
  scroll_box(height = "320px")
```

```{r wordstemcloud, dpi=150, echo=FALSE, fig.cap="Wordstem cloud of AGILE 2020 full paper submissions"}
wordcloud(cloud_wordstems$wordstem, cloud_wordstems$n,
          max.words = Inf,
          random.order = FALSE,
          fixed.asp = FALSE,
          rot.per = 0,
          color = brewer.pal(8,"Dark2"))
```

## Reproducible research-related keywords of all submissions

The following tables lists how often terms related to reproducible research appear in each document.
The detection matches full words using regex option `\b`.

- reproduc (`reproduc.*`, reproducibility, reproducible, reproduce, reproduction)
- replic (`replicat.*`, i.e. replication, replicate)
- repeatab (`repeatab.*`, i.e. repeatability, repeatable)
- software
- (pseudo) code/script(s) [column name _code_]
- algorithm (`algorithm.*`, i.e. algorithms, algorithmic)
- process (`process.*`, i.e. processing, processes, preprocessing)
- data (`data.*`, i.e. dataset(s), database(s))
- result(s) (`results?`)
- repository(ies) (`repositor(y|ies)`)

```{r keywords_per_paper, echo=FALSE, warning=FALSE}
tidy_texts_lower <- str_to_lower(tidy_texts$text)
word_counts <- tibble(
  id = tidy_texts$id,
  type = tidy_texts$type,
  DASA = tidy_texts$has_dasa,
  `reproduc..` = str_count(tidy_texts_lower, "\\breproduc.*\\b"),
  `replic..` = str_count(tidy_texts_lower, "\\breplicat.*\\b"),
  `repeatab..` = str_count(tidy_texts_lower, "\\brepeatab.*\\b"),
  `code` = str_count(tidy_texts_lower,
    "(\\bcode\\b|\\bscript.*\\b|\\bpseudo\ code\\b)"),
  software = str_count(tidy_texts_lower, "\\bsoftware\\b"),
  `algorithm(s)` = str_count(tidy_texts_lower, "\\balgorithm.*\\b"),
  `(pre)process..` = str_count(tidy_texts_lower, 
                "(\\bprocess.*\\b|\\bpreprocess.*\\b|\\bpre-process.*\\b)"),
  `data.*` = str_count(tidy_texts_lower, "\\bdata.*\\b"),
  `result(s)` = str_count(tidy_texts_lower, "\\bresults?\\b"),
  `repository/ies` = str_count(tidy_texts_lower, "\\brepositor(y|ies)\\b")
)

# https://stackoverflow.com/a/32827260/261210
sumColsInARow <- function(df, list_of_cols, new_col) {
  df %>% 
    mutate_(.dots = ~Reduce(`+`, .[list_of_cols])) %>% 
    setNames(c(names(df), new_col))
}

word_counts_sums <- sumColsInARow(
  word_counts, 
  names(word_counts)[!(names(word_counts) %in% c("id", "type"))], "all") %>%
  arrange(desc(all))

word_counts_sums_total <- word_counts_sums %>% 
  summarise_if(is.numeric, funs(sum)) %>%
  add_column(id = "Total", type = "", DASA = "", .before = 0)
word_counts_sums <- rbind(word_counts_sums, word_counts_sums_total)

word_counts_sums %>%
  kable() %>%
  kable_styling("striped", font_size = 12, bootstrap_options = "condensed")  %>%
  row_spec(0, font_size = "x-small", bold = T)  %>%
  row_spec(word_counts_sums %>% rownames_to_column() %>%
             filter(DASA == TRUE, .preserve = TRUE) %>%
             select(rowname) %>% unlist() %>% as.numeric(),
           italic = TRUE, background = "#eeeeee") %>%
  row_spec(nrow(word_counts_sums), bold = T)
```

## Accepted full papers

### Full paper decisions

There is "accept" and "conditionally accept" (after second review)!

```{r scrape_accepted, echo=FALSE}
submission_table %>%
  filter(type == "Full-paper") %>%
  group_by(decision) %>%
  summarise(count = n()) %>%
  kable() %>%
  kable_styling("striped")
```

```{r compile review data}
#page <- httr::GET(url = "https://easychair.org/conferences/status?a=24268236")
#review_status_page <- xml2::read_html(page)
#review_table <- rvest::html_table(html_nodes(review_status_page, ".paperTable")[[1]], header = TRUE)
#names(review_table)[4] <- "average"
#names(review_table)[1] <- "id"
#review_table$id <- str_pad(review_table$id, width = 3, side = "left", pad = "0")
#
## IMPORTANT: "Show paper authors" must be _un_ticked for the following code to work
#review_table <- review_table %>%
#  tidyr::separate(col = title, into = c("authors","title"), sep = "\\.+?", extra = "merge")
#
## the tr element of the review table has the internal paper ID in format "r4789577"
#review_table$internal_id <- sapply(X = html_nodes(review_status_page, css = ".paperTable tr[id]"), FUN = function(row) {
#  substr(html_attr(row, "id"), 2, 999)
#})

review_data <- left_join(submission_table, dasa_texts %>% select(-type),
                         by = "id")

review_data %>%
  dplyr::filter(decision == "ACCEPT" | decision == "accept?") %>%
  filter(type == "Full-paper") %>%
  arrange(id) %>%
  kable() %>%
  kable_styling("striped") %>%
  scroll_box(height = "320px")
```

### How does acceptance relate to DASA section availability for full papers?

```{r accepted_dasa, echo=FALSE}
dasa_vs_accepted <- tibble(papers = c("submitted",
                  "submitted with DASA",
                  "accepted",
                  "accepted with DASA",
                  "conditionally accepted",
                  "conditionally accepted with DASA",
                  "rejected with DASA"),
       n = c(review_data %>%
               filter(type == "Full-paper") %>%
               nrow(.),
             review_data %>%
               filter(type == "Full-paper") %>%
               filter(!is.na(dasa_text)) %>% 
               nrow(.),
             review_data %>%
               dplyr::filter(decision == "ACCEPT") %>%
               filter(type == "Full-paper") %>%
               nrow(.),
             review_data %>%
               dplyr::filter(decision == "ACCEPT") %>%
               filter(type == "Full-paper") %>%
               filter(!is.na(dasa_text)) %>% 
               nrow(.),
             review_data %>%
               dplyr::filter(decision == "accept?") %>%
               filter(type == "Full-paper") %>%
               nrow(.),
             review_data %>%
               dplyr::filter(decision == "accept?") %>%
               filter(type == "Full-paper") %>%
               filter(!is.na(dasa_text)) %>% 
               nrow(.),
             review_data %>%
               dplyr::filter(decision == "REJECT") %>% 
               filter(!is.na(dasa_text)) %>% 
               nrow(.))
)
dasa_vs_accepted %>%
  kable() %>%
  kable_styling("striped")
```

------

```{r accepted_dasa_barplot}
review_data %>%
  mutate(`has DASA` = !is.na(dasa_text)) %>%
  filter(!is.na(decision)) %>%
  #filter(decision != "REJECT") %>%
  filter(type == "Full-paper") %>%
  group_by(type, `has DASA`, decision) %>%
  summarise(n = n()) %>%
  ggplot(aes(x=decision, y=n, fill=`has DASA`)) +
  geom_bar(stat="identity") + 
  scale_fill_brewer(palette="Paired") +
  labs(title = "AGILE 2020 Submissions Transparency & Reproducibility",
       subtitle = "Data and Software Availability Sections (DASA) across accepted, conditionally accepted, and\nrejected full paper submissions") +
  theme_minimal()
```

### How are the reviewer's comments regarding reproducibility?

- Do reviewers estimates match the automatic detection of DASA?
- How many comments are there regarding reproducibility

### Reproducibility Reviewer "Get started" information

```{r review_contents}
# TODO for each accepted paper, print the title, DASA section excerpt, the reviewer comments, filename, and responsible reviewer initials
```

## License & Metadata

This document is licensed under a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).
All contained code is licensed under the [Apache License 2.0](https://choosealicense.com/licenses/apache-2.0/).

**Runtime environment description:**

```{r session_info}
devtools::session_info(include_base = TRUE)
```

```{r upload_to_drive, eval=FALSE, include=FALSE}
# upload the HTML file and source code to the Reproducibility Committee shared folder
drive_put("agile-2020-papers.html", path = "https://drive.google.com/drive/folders/1jyYj1hFqbR74D9ljjjcScR4lD3aWCO2U/")
drive_put("agile-2020-papers.Rmd", path = "https://drive.google.com/drive/folders/1jyYj1hFqbR74D9ljjjcScR4lD3aWCO2U/")
```
